{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "FKbD37zmGhFn"
      },
      "outputs": [],
      "source": [
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "import json \n",
        "import csv \n",
        "import numpy as np"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class scrapper :\n",
        "  result = list()\n",
        "  urls =list()\n",
        "  def fetch(self,url):\n",
        "    responce= requests.get(url)\n",
        "    return responce\n",
        "\n",
        "  def parse(self,html):\n",
        "    content = BeautifulSoup(html,\"lxml\")\n",
        "    title = [title.text for title in content.find_all(\"h2\",{\"class\":\"home-title\"})]\n",
        "    links= [link[\"href\"]for link in content.find_all(\"a\",{\"class\":'story-link'})]\n",
        "    Dates =[date.text.strip(\"\\n\")[1:] for date in content.find_all(\"span\",{\"class\":\"h-datetime\"})]\n",
        "    authors =[aut.text.strip(\"\\n\") for aut in content.find_all(\"span\",{\"class\":\"h-tags\"})]\n",
        "    descriptions =[desc.text for desc in content.find_all(\"div\",{\"class\":\"home-desc\"})]\n",
        "    next_page = content.find(\"a\",{\"class\":\"blog-pager-older-link-mobile\"}) [\"href\"]\n",
        " \n",
        "\n",
        "    self.urls.append(next_page) \n",
        "    for idx in range(len(title)):\n",
        "       self.result.append({\n",
        "          \"titles\":title[idx] ,\n",
        "          \"links\":links[idx],\n",
        "          \"descriptions\":descriptions[idx],\n",
        "          \"Dates\":Dates[idx]if len(Dates) > idx else [] ,\n",
        "          \"authors\":authors[idx] if len(authors) > idx else [] })\n",
        "       \n",
        "  def to_csv(self):\n",
        "    with open(\"result.csv\",\"w\") as csvfile:\n",
        "      writer = csv.DictWriter(csvfile, fieldnames= self.result[0].keys() )\n",
        "      writer.writeheader()\n",
        "      for row in self.result:\n",
        "        writer.writerow(row)\n",
        "      print(\"stored\")      \n",
        "\n",
        "\n",
        "\n",
        "\n",
        "  def run(self):\n",
        "    ''' responce = self.fetch(\"https://thehackernews.com/\")\n",
        "     with open('res.html','w') as html_file:\n",
        "      html_file.write(responce.text)\n",
        "\n",
        "    html =''\n",
        "    with open('res.html','r') as html_file:\n",
        "      for line in html_file.read():\n",
        "        html += line    '''\n",
        "    self.urls.append(\"https://thehackernews.com/\")\n",
        "    for i  in range(0,5):\n",
        "      responce= self.fetch(self.urls[-1])\n",
        "      html = responce.text\n",
        "      self.parse(html)\n",
        "\n",
        "    self.to_csv()\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "if __name__==\"__main__\":\n",
        "  scrape = scrapper()\n",
        "  scrape.run()\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6XphBQQLHVUP",
        "outputId": "d086111b-be39-4b0d-bf6d-53c012ef288f"
      },
      "execution_count": 48,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "stored\n"
          ]
        }
      ]
    }
  ]
}